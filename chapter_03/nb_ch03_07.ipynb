{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nb_ch03_07.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O2sP8Kwe7L9Z"
      },
      "source": [
        "## Fitting a linear regression model with Autograd\n",
        "\n",
        "**Goal:** In this notebook you will see how to use the Autograd library from python to fit the parameters (slope and intercept) of a simple linear regression model via gradient descent (GD). \n",
        "\n",
        "**Usage:** The idea of the notebook is that you try to understand the provided code by running it, checking the output and playing with it by slightly changing the code and rerunning it. \n",
        "\n",
        "**Dataset:** You work again with the systolic blood pressure and age data of 33 American women, which is generated and visualized in the upper part of the notebook. \n",
        "\n",
        "**Content:**\n",
        "\n",
        "* fit a linear model via the sklearn machine learning library of python to get the fitted values of the intercept and slope as reference. \n",
        "\n",
        "* use the autograd library and the contained *grad* function to fit the parameters of the simple linear model via GD with the objective to minimize the MSE loss. \n",
        "    * define the mse loss function \n",
        "    * determine the gradients of the loss w.r.t. the parameters via automatic differentiation\n",
        "    * use these gradients to update the parameter values via the update formula\n",
        "    * iterate over the two former steps for many steps and check the current values of the estimated model parameters and the loss after each updatestep \n",
        "    * verify that the estimated parameter values converge to the values which you got from the sklearn fit.  \n",
        "\n",
        "\n",
        "\n",
        "[open in colab](https://colab.research.google.com/github/tensorchiefs/dl_book/blob/master/chapter_03/nb_ch03_07.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RMlU4PJJ5QO7",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('default')\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p3c9bh7zMVhP"
      },
      "source": [
        "Here we read in the systolic blood pressure and the age of the 33 American women in our dataset. Then we use the sklearn library to find the optimal values for the slope a and the intercept b."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RvINwW1vydo9",
        "colab": {}
      },
      "source": [
        "# Blood Pressure data\n",
        "x = [22, 41, 52, 23, 41, 54, 24, 46, 56, 27, 47, 57, 28, 48, 58,  9, \n",
        "     49, 59, 30, 49, 63, 32, 50, 67, 33, 51, 71, 35, 51, 77, 40, 51, 81]\n",
        "y = [131, 139, 128, 128, 171, 105, 116, 137, 145, 106, 111, 141, 114, \n",
        "     115, 153, 123, 133, 157, 117, 128, 155, 122, 183,\n",
        "     176,  99, 130, 172, 121, 133, 178, 147, 144, 217] \n",
        "x = np.asarray(x, np.float32) \n",
        "y = np.asarray(y, np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OurbybEoydpB",
        "outputId": "8e1c7227-a342-4fb1-8813-e906ab50fa2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "plt.scatter(x=x,y=y)\n",
        "plt.title(\"blood pressure vs age\")\n",
        "plt.xlabel(\"x (age)\")\n",
        "plt.ylabel(\"y (sbp)\")\n",
        "\n",
        "model = LinearRegression()\n",
        "res = model.fit(x.reshape((len(x),1)), y)\n",
        "predictions = model.predict(x.reshape((len(x),1)))\n",
        "plt.plot(x, predictions)\n",
        "plt.show()\n",
        "print(\"intercept = \",res.intercept_,\"solpe = \", res.coef_[0],)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "intercept =  87.67143 solpe =  1.1050216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qDre_3MNSFtJ"
      },
      "source": [
        "## Autograd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pu3bqKzRDQqw"
      },
      "source": [
        "Now we want to use Autograd, a library for automatic differentiation. First we need to install it. Then we again can define our mse loss and calculate the minimal loss with the optimal values for the slope a and the inercept b from above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gas_UQxrDOFh",
        "outputId": "b0173f9b-8127-4641-ef2e-51277bda9736",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!pip install autograd #You might need to install autograd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: autograd in /usr/local/lib/python3.6/dist-packages (1.3)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from autograd) (0.18.2)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from autograd) (1.17.2)\n",
            "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "syWi2amOR-pE",
        "colab": {}
      },
      "source": [
        "import autograd.numpy as np\n",
        "from autograd import grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jyunhm7wR-9G",
        "colab": {}
      },
      "source": [
        "def loss(a,b):\n",
        "  y_hat = a*x + b\n",
        "  return np.sum((y_hat - y)**2) / len(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yaLCW1AcX6pr",
        "outputId": "6d4be376-7c45-4a14-9abc-1a510f178411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "loss(1.1050216,87.67143) #minimal loss for the optimal valuese for slope a and intercept b"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "349.2007871685606"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OU3rc92DEZmv"
      },
      "source": [
        "Now we define that we want to have the gradients of the loss w.r.t to our two model parameters, the slope a and the intercept b. In the nex cell we print the gradient of the loss w.r.t to a and gradient of the loss w.r.t to b. Note that we calculated the loss for all data points and therefore we get diffrent gradients then in nb_04, where we only used one datapoint. Autograds *grad* function, takes a function as input and returs a function that computes its derivative. You can use the derivative function to compute the gradient at a specific position of the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lltyH-eRUPSn",
        "outputId": "81d14be0-77e5-4a4a-cf6a-ac972b36e402",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "grad_loss_a = grad(loss,0)\n",
        "grad_loss_b = grad(loss,1)\n",
        "print(grad_loss_a(0.,139.))\n",
        "print(grad_loss_b(0.,139.))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-553.0909090909091\n",
            "0.7272727272727257\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nrmK9PP0El3L"
      },
      "source": [
        "Now, let's use gradient descent to optimize the slope a and the intercept b. The start values are a=0 and b=139  (139 is the mean of the blood pressure and slope a=0 implies that the model predicts the mean for each age). Our learning rate eta is 0.0004 and we do 80000 updatesteps with all 33 observations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8odVy_28U39q",
        "outputId": "0aae3fc9-d83b-472b-d535-906cf29058af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "eta = 0.0004\n",
        "a = 0.0\n",
        "b = 139.0\n",
        "for i in range(80000):\n",
        "    grad_a, grad_b  = grad_loss_a(a,b),grad_loss_b(a,b)\n",
        "    a = a - eta * grad_a\n",
        "    b = b - eta * grad_b\n",
        "    if (i % 5000 == 0):\n",
        "      print(\"Epoch:\",i, \"slope=\",a,\"intercept=\",b,\"gradient_a\", grad_a, \"gradient_b\",grad_b, \"mse=\", loss(a,b))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 slope= 0.22123636363636365 intercept= 138.99970909090908 gradient_a -553.0909090909091 gradient_b 0.7272727272727257 mse= 668.1919389204545\n",
            "Epoch: 5000 slope= 0.47009054898348496 intercept= 120.60789269261207 gradient_a -0.1413740678267459 gradient_b 7.305982185132575 mse= 469.551787405303\n",
            "Epoch: 10000 slope= 0.6975811188668976 intercept= 108.8070221024948 gradient_a -0.09035931211531079 gradient_b 4.688314726858429 mse= 398.7600615530303\n",
            "Epoch: 15000 slope= 0.8435640896883931 intercept= 101.23430788222818 gradient_a -0.059386282256184586 gradient_b 3.008502382220645 mse= 369.60884232954544\n",
            "Epoch: 20000 slope= 0.9372421279444895 intercept= 96.37483582948659 gradient_a -0.03760736638852791 gradient_b 1.9305910052675181 mse= 357.6046401515151\n",
            "Epoch: 25000 slope= 0.9973560807661642 intercept= 93.25647324940135 gradient_a -0.023206306226256856 gradient_b 1.2388925263375947 mse= 352.6614583333333\n",
            "Epoch: 30000 slope= 1.0359316219445518 intercept= 91.25539471546622 gradient_a -0.014059355764715065 gradient_b 0.7950245250355108 mse= 350.6258581912879\n",
            "Epoch: 35000 slope= 1.0606859750458772 intercept= 89.97128639452472 gradient_a -0.008185646750661135 gradient_b 0.5101887096058237 mse= 349.78767163825756\n",
            "Epoch: 40000 slope= 1.0765710319750297 intercept= 89.1472636781643 gradient_a -0.004884199662683386 gradient_b 0.3273990515506626 mse= 349.44247159090907\n",
            "Epoch: 45000 slope= 1.086764409105711 intercept= 88.61848164561941 gradient_a -0.00238684451940685 gradient_b 0.21010982629024655 mse= 349.30036695075756\n",
            "Epoch: 50000 slope= 1.0933061270049231 intercept= 88.27915791866468 gradient_a -0.002976851029870886 gradient_b 0.1348016912286929 mse= 349.2418323863636\n",
            "Epoch: 55000 slope= 1.0975039356578573 intercept= 88.06141114039603 gradient_a -0.003163655598996229 gradient_b 0.08647849343039882 mse= 349.21771425189394\n",
            "Epoch: 60000 slope= 1.1001973614317064 intercept= 87.9216812918 gradient_a -0.0012507583156491364 gradient_b 0.05551008744673336 mse= 349.2078598484849\n",
            "Epoch: 65000 slope= 1.1019258319276868 intercept= 87.83201547870098 gradient_a -0.000584457859901022 gradient_b 0.035625110973010354 mse= 349.2037168560606\n",
            "Epoch: 70000 slope= 1.1030352583451921 intercept= 87.77447621330529 gradient_a -0.0010107791785287645 gradient_b 0.022847955877130843 mse= 349.20205965909093\n",
            "Epoch: 75000 slope= 1.1037471182158671 intercept= 87.73755278209346 gradient_a -0.0011222145773785996 gradient_b 0.014652598987926702 mse= 349.2013790246212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nBEvvFxtFvg6"
      },
      "source": [
        "Let's look at the final values for the slope a, the intercept b and the mse loss. We know form the closed formula solution that:\n",
        "\n",
        "1.   optimal value for a: 1.1050216\n",
        "2.   optimal value for b: 87.67143\n",
        "3.   minimal loss: 349.200787168560\n",
        "\n",
        "After 80000 update steps we are very close to the optimal values\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "omFXVvFIW3aT",
        "outputId": "7b59f33d-695f-4563-8f30-94cde06778b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(a,b, loss(a,b))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1042025997277585 87.71386248723839 349.20108309659093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0E8x7MILnDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}